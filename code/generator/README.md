# Dataset Views Generator

This folder contains the code to generate the dataset views from the original datasets.

## Install Dependencies

You need to install the dependencies listed in the `requirements.txt` file. You can do it by running the following command:

```bash
pip install -r requirements.txt
```

You also must have the datasets downloaded and run the python file `dataset_generator` that is used to generate the dataset views.

## Datasets Directory Structure

From the root's git repository path, the datasets must be placed in the `preliminary_analisys/datasets_preprocessing/data` directory.
The datasets must have the following structure:

```
preliminary_analisys
└── datasets_preprocessing
    └── data
        ├── original
        │   ├── KuHar
        │   ├── MotionSense
        │   ├── RealWord
        │   ├── UCI
        │   └── WISDM
        └── raw_balanced
        │   ├── KuHar
        │   ├── MotionSense
        │   ├── RealWord
        |   ├── RealWorld_thigh
        |   ├── RealWorld_waist
        │   ├── UCI
        │   └── WISDM
        └── standartized_balanced
        │   ├── KuHar
        │   ├── MotionSense
        │   ├── RealWord
        |   ├── RealWorld_thigh
        |   ├── RealWorld_waist
        │   ├── UCI
        │   └── WISDM
        └── unbalanced
            ├── KuHar
            ├── MotionSense
            ├── RealWord
            ├── RealWorld_thigh
            ├── RealWorld_waist
            ├── UCI
            └── WISDM
```

Where the `original` directory contains the original datasets (as they are downloaded), the `raw_balanced` will be generated by the `dataset_generator.py` and the directory contains the raw balanced datasets, that is, without standardization. The `standartized_balanced` directory contains the standardized balanced datasets and the `unbalanced` directory contains the unbalanced datasets, that is, standardized datasets, but before train/validation/test splits.

The RealWorld, RealWorld_thigh, and RealWorld_waist directories have the following difference:

- RealWorld: The entire dataset is pre-processed

- RealWorld_thigh: The dataset is filtered by samples with position equal to the thigh before doing the pre-processing

- RealWorld_waist: The dataset is filtered by samples with position equal to the waist before doing the pre-processing

---

**NOTE**

- To generate datasets, the original data must be placed in the `preliminary_analisys/datasets_preprocessing/data/original` directory.
- The `raw_balanced` and `standartized_balanced` directories will be generated by the `dataset_generator.py`.
- Do not add the datasets to the git repository. The datasets are too big, and it is not necessary to add them to the repository. The datasets can be downloaded from the sources or downloaded from [HIAAC M4 Datasets Drive](https://drive.google.com/drive/u/1/folders/1NF63hQu1hCpVU2GlxjLuE5EcxYm9i0EP). You can download the file original.zip file if you want to generate the views.
- The `preliminary_analisys/datasets_preprocessing/data` directory is ignored by git.

---

## Dataset Generation Process

To generate the datasets, you can run the `dataset_generator.py` and `inter_dataset_balancer.py` files, which is in this directory. This file generates the `raw` and `standartized` views that can be unbalanced, intra_balanced (balanced per activity), and inter_balanced (balanced per activity and user), that is, all users have the same number of samples per activity. For generate inter_balanced views,  that balances each dataset based the minimum number of classes per split (train, validation or test), of all datasets, you must run the file `inter_dataset_balancer.py`.

The order is very important, to run `inter_dataset_balancer.py` you need the intra_balanced datasets, so if you don't have these views on your machine, you should first run `dataset_generator.py` file and then run the `inter_dataset_balancer.py` file.

## Standardization Process

The standardization process also called the standardization pipeline, comprises the execution of several steps (operations) per dataset.
Each dataset has its standardization pipeline, as they are different from each other. 
The operators are all defined in the `dataset_processor.py` file. The operators are defined as classes, and each class has a `__call__` method, which receives a pandas dataframe and returns a pandas dataframe. The `__call__` method is the operator's implementation.

---

**NOTE**

- The order of the operators is important, as some operators may require columns that may be added from other operators (which must run before).
- Seldom, some operators may return multiple pandas Dataframes. 

---

The standardization codes from `dataset_generator.py` file usually comprise the following steps (**this is not a strict rule**):

1. Load the datasets and generate a single pandas dataframe with all the data, where each row represents a single instant of capture (and now a window). The loading is a dataset-specific process. The dataframe generated **usually** have the following columns (**this is not a rule**):
- A column for the x-axis acceleration (`accel-x` column); y-axis acceleration (`accel-y` column); z-axis acceleration (`accel-z` column); gyroscope x-axis (`gyro-x` column); gyroscope y-axis (`gyro-y` column); gyroscope z-axis (`gyro-z` column); and the timestamp from the accelerometer (`accel-timestamp`) and gyroscope (`gyro-timestamp`), if provided.
- A column for the label (`activity code` column).
- A column for the user id (`user` column), if provided.
- A column for x-axis gravity (`gravity-x` column); y-axis gravity (`gravity-y` column); and z-axis gravity (`gravity-z` column) if provided.
- A serial column, which represents the attempt that the collection was made (`serial` column), if provided. For instance, if the user has a time series running in the morning and another in the afternoon, it will be two different serial numbers.
- A CSV or file column, which represents the file that the row was extracted from (`csv` column). 
- An index column, that is, a column that represents the row index from the CSV file (`index` column).
- Any other column that may be useful for the standardization process or metadata.
2. Create the operator objects.
3. Crete the pipeline object, passing the operator object list as parameters.
4. Execute the pipeline, passing the dataframe as a parameter.

The following code snippet illustrates a fictitious standardization process for the `KuHar` dataset that resamples it to 20Hz and creates 3-second windows (**using this kind of code and operators is not a rule**):

```python

def read_kuhar(kuhar_dir_path: str) -> pd.DataFrame:
    # This is dataset-specific code. It reads the CSV files and generates a single dataframe with all the data.
    ... 
    return dataframe

# -----------------------------------------------------------------------------
# 1. Load the datasets and generate a single pandas dataframe with all the data
# -----------------------------------------------------------------------------

dataframe = read_kuhar("../data/original/KuHar/1.Raw_time_domian_data")

# -----------------------------------------------------------------------------
# 2. Create operaators
# -----------------------------------------------------------------------------

# List with columns that are features
feature_columns = [
    "accel-x",
    "accel-y",
    "accel-z",
    "gyro-x",
    "gyro-y",
    "gyro-z",
]


# Instantiate the object that resample the data to 20Hz
# (supose that the original dataset has a constant sample rate equal to 100Hz)
resampler = ResamplerPoly(
    features_to_select=feature_columns, # Name of the columns that will be used
                                        # as features.
    up=2,                               # The upsampling factor.
    down=10,                            # The downsampling factor.
    groupby_column="csv",               # Group by csv column. 
                                        # Resampling is done for each group of
                                        # csv column group.
)

# Instantiate the object taht creates the windows
windowizer = Windowize(
    features_to_select=feature_columns, # Name of the coluns that will be used
                                        # as features.
    samples_per_window=60,              # Number of samples per window.
    samples_per_overlap=0,              # Number of samples that overlap.
    groupby_column="csv",               # Group by csv column.
                                        # Resampling is done for each group of
                                        # csv column group.
)

# -----------------------------------------------------------------------------
# 3. Create the pipeline object, passing the operator object list as parameters
# -----------------------------------------------------------------------------

# Create the pipeline
# 1. Resample the data
# 2. Create the windows
# 3. Add a column wit activity code
pipeline = Pipeline(
    [
        differ,
        resampler,
        windowizer,
        standard_label_adder
    ]
)

# -----------------------------------------------------------------------------
# 4. Execute the pipeline, passing the dataframe as a parameter
# -----------------------------------------------------------------------------

standartized_dataset = pipeline(dataframe)
```

The example above is to read and preprocess a dataset, in the `dataset_generator.py` file there a dictionary with a specific pipeline for each dataset and view. 

The pipeline operators are usually shared with other datasets, as they are generic.